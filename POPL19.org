#+TITLE: Theories and Data Structures
#+SUBTITLE: ---Draft---
#+DESCRIPTION: Work done at McMaster University, 2019.
#+AUTHOR: [[mailto:alhassm@mcmaster.ca][Musa Al-hassy]], [[mailto:carette@mcmaster.ca][Jacques Carette]], [[mailto:kahl@cas.mcmaster.ca][Wolfram Kahl]]
#+EMAIL: alhassy@gmail.com
#+EMAIL: carette@mcmaster.ca
#+OPTIONS: toc:nil d:nil
#+PROPERTY: header-args :tangle no :comments link

#+TODO: TODO | STARTED OLD  | DONE

# Top level editorial comments.
#+latex_header: \def\edcomm#1#2{ \fbox{\textbf{Comment: #1 }} \emph{#2} \fbox{\textbf{End Comment}}}

# Use:  x vs.{{{null}}} y
# This informs LaTeX not to put the normal space necessary after a period.
#
#+MACRO: null  @@latex:\null{}@@

#+MACRO: edcomm  @@latex:\edcomm{$1}{$2}@@
#
# Warning: {{{edcomm(this, that)}}} cannot contain any commas in â€˜thisâ€™ nor in â€˜thatâ€™!

* Preamble :ignore:

# Top level editorial comments.
#+LATEX: \def\edcomm#1#2{ \fbox{\textbf{Comment: #1 }} #2 \fbox{\textbf{End Comment}}}

#+LATEX_HEADER: \usepackage[]{minted}
#+LaTeX: \setminted[haskell]{fontsize=\footnotesize}
# Removing the red box that appears in "minted" when using unicode.
# Src: https://tex.stackexchange.com/questions/343494/minted-red-box-around-greek-characters
#
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \AtBeginEnvironment{minted}{\dontdofcolorbox}
#+LATEX_HEADER: \def\dontdofcolorbox{\renewcommand\fcolorbox[4][]{##4}}
#+LATEX_HEADER: \makeatother

#+LATEX_HEADER: \usepackage{multicol}
#+NAME: parallel enviro
#+BEGIN_EXPORT latex
\renewenvironment{parallel}[1][2]
 {
  \setlength{\columnseprule}{2pt}
  \begin{minipage}[t]{\linewidth}
  \begin{multicols}{#1}
 }
 {
 \setlength{\columnseprule}{0pt}
  \end{multicols}
  \end{minipage}
 }
#+END_EXPORT

** LaTeX setup                                                      :ignore:
# latex_class_options: [acmsmall,review,anonymous]
#+LATEX_CLASS: acmart

#+LATEX_HEADER: \settopmatter{prinfolios=true,princcs=false,printacmref=false}
#+LATEX_HEADER: \usepackage[backend=biber,style=alphabetic]{biblatex}
#+LATEX_HEADER: \addbibresource{MyReferences.bib}

#+LATEX_HEADER: \acmJournal{PACMPL}
#+LATEX_HEADER: \acmVolume{1}
#+LATEX_HEADER: \acmNumber{POPL}
#+LATEX_HEADER: \acmArticle{1}
#+LATEX_HEADER: \acmYear{2020}
#+LATEX_HEADER: \acmMonth{1}
#+LATEX_HEADER: \acmDOI{}
#+LATEX_HEADER: \setcopyright{none}

#+LATEX_HEADER: \usepackage{MyUnicodeSymbols}
#+LATEX_HEADER: \newunicodechar{ğ’«}{\ensuremath{\mathcal{P}}}
#+LATEX_HEADER: \newunicodechar{â¨¾}{\ensuremath{\mathop{\fatsemi}}}
#+LATEX_HEADER: \newunicodechar{Î£}{\ensuremath{\mathop{\Sigma}}}
#+LATEX_HEADER: \newunicodechar{âˆ˜}{\ensuremath{\mathop{\circ}}}
#+LATEX_HEADER: \newunicodechar{Î“}{\ensuremath{\Gamma}}
#+LATEX_HEADER: \newunicodechar{Î }{\ensuremath{\Pi}}
#+LATEX_HEADER: \newunicodechar{âŸ¦}{\ensuremath{\llbracket}}
#+LATEX_HEADER: \newunicodechar{âŸ§}{\ensuremath{\rrbracket}}
#+LATEX_HEADER: \newunicodechar{Î˜}{\ensuremath{\theta}}
#+LATEX_HEADER: \newunicodechar{âˆ}{\ensuremath{\qedsymbol}}
#+LATEX_HEADER: \newunicodechar{â€²}{'}
#+LATEX_HEADER: \newunicodechar{Ï„}{\ensuremath{\tau}}
#+LATEX_HEADER: \newunicodechar{â¦ƒ}{\ensuremath{ \{\{ }}  % this is not correct
#+LATEX_HEADER: \newunicodechar{â¦„}{\ensuremath{ \}\} }}   % this is not correct
#+LATEX_HEADER: \newunicodechar{âŠ}{\ensuremath{\cupdot}}  % should be in myunicode; go #regenerate# it!
#+LATEX_HEADER: \def\with{\kern0.7em \withrule \kern0.7em }
#+LATEX_HEADER: \def\withrule{\vrule height1.57ex depth0.43ex width0.12em}
#+LATEX_HEADER: \newunicodechar{â™}{\ensuremath{\mathop{\with}}}

#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor} % named colours
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.3, 0.0, 0.0}
#+LATEX_HEADER: \definecolor{darkgreen}{rgb}{0.0, 0.3, 0.1}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0, 0.1, 0.3}
#+LATEX_HEADER: \definecolor{darkorange}{rgb}{1.0, 0.55, 0.0}
#+LATEX_HEADER: \definecolor{sienna}{rgb}{0.53, 0.18, 0.09}
#+LATEX_HEADER: \hypersetup{colorlinks,linkcolor=darkblue,citecolor=darkblue,urlcolor=darkgreen}

#+NAME: symbols for itemisation environment
#+BEGIN_EXPORT latex
\def\labelitemi{$\diamond$}
\def\labelitemii{$\circ$}
\def\labelitemiii{$\star$}
#+END_EXPORT

# Having small-font code blocks.
# LATEX_HEADER: \RequirePackage{fancyvrb}
# LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\scriptsize}
#+BEGIN_EXPORT latex
\author{Musa Al-hassy}
\affiliation{
  \institution{McMaster University}
  \streetaddress{1280 Main St. W.}
  \city{Hamilton}
  \state{ON}
  \postcode{L8S 4K1}
  \country{Canada}}
\email{alhassym@mcmaster.ca}
\author{Jacques Carette}
\author{Wolfram Kahl}
#+END_EXPORT

** COMMENT acmart Emacs setup
#+NAME: make-acmart-class
#+BEGIN_SRC emacs-lisp :results none
(with-eval-after-load "ox-latex"
   (add-to-list 'org-latex-classes
        '("acmart" "\\documentclass{acmart}"
          ("\\section{%s}" . "\\section*{%s}")
          ("\\subsection{%s}" . "\\subsection*{%s}")
          ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
          ("\\paragraph{%s}" . "\\paragraph*{%s}")
          ("\\subparagraph{%s}" . "\\subparagraph*{%s}"))))
#+END_SRC
* COMMENT Abstract :ignore:

  #+begin_center
  *Abstract*
  #+end_center
  #+begin_small
  The ubiquitous data structures found in computing are accompanied by a core interface
  for their manipulation. Irrespective of the language, this core interface arises naturally.

  With a bit of elementary mathematics, we demonstrate that this is no accident: Data structures
  provide abstract syntax trees corresponding to meaningful conceptual theories and the coherency of the
  interface is nothing more than the structure abiding by the laws of the theory.
  #+end_small

* COMMENT Introduction

  Our story begins with abstract syntax trees.

  Programmers write code, which they may want to execute remotely and so they serialise it
  in a coherent format, transmit it, then have it re-interpreted as meaningful code and executed.
  We will demonstrate some concepts in an ambient language, discuss what it means for them to be coherent,
  show that their associated â€˜freeâ€™ structures provide a means to serialise them, then demonstrate that
  the proof obligations for â€˜freeâ€™ necessitate an interpretation, or compilation, function.

  The essence of our exposition is the adjunction, which requires understanding naturality, functoriality,
  and categories. In section 2, we review these basic concepts to make this work accessible; then we illustrate the process of obtaining data structures from theories by considering adjunctions.
  Section 3 demonstrates this process by obtaining concepts such as for-loops from pointed unary theories
  and section 4 demonstrates how linked-lists and their interface are obtained from the theory of monoids.
  Finally, section 5 concludes by providing a tabulation of a number of theories and their corresponding
  data structures.

  Our exposition will be informal for brevity and accessibility, however the fact that the proof obligations
  /forced/ the common interfaces to arise was due to exploration in mechanising theories in the proof assistant Agda,
  which did not allow us to elide any â€œtrivial detailsâ€. The full details of the theories mentioned in section 5
  can be found at: https://github.com/JacquesCarette/TheoriesAndDataStructures

* COMMENT Compositionality Schemes

  â€˜Stepwise refinementâ€™ is the idea that programming beings with the
  empty, or do-nothing, program ~skip~ then new programs ~S â¨¾ T~ are formed
  by sequencing other programs together. Moreover, sequencing is associative
  ---that is, ~(S â¨¾ T) â¨¾ R = S â¨¾ (T â¨¾ R)~--- and so languages, such as C or Python, do not require parentheses.
  This is an instance of a /monoid/, which is a structure that consists of a collection called the ~Carrier~
  and operation ~_â¨¾_ : Carrier â†’ Carrier â†’ Carrier~ which is associative and has a no-op identity ~Id : Carrier~.

  In strongly typed languages we cannot just sequence arbitrary programs as we would in, say,
  Python, only to obtain an incompatibility error at runtime. Instead, we would like to be able
  to discuss sequencing â€˜well-typedâ€™ programs. This naturally gives rise to /categories/, which
  consist of a collection called ~Objects~ ---the â€˜typesâ€™ of the language--- and for each
  objects ~A, B~ a collection ~A âŸ¶ B~ of â€œ morphisms, programs, from ~A~ to ~B~ â€
  ---sometimes also denoted ~Hom A B~--- such that compatible morphisms can be composed to yield
  new morphisms by means of an operation ~_â¨¾_ : âˆ€{A B C} â†’ (A âŸ¶ B) â†’ (B âŸ¶ C) â†’ (A âŸ¶ C)~
  which is associative and has a no-op identity ~Id : âˆ€{A} â†’ A âŸ¶ A~.
  We are using a long arrow â€˜âŸ¶â€™ to refer to morphisms, programs, of a category and
  a short arrow â€˜â†’â€™ to refer to usual function typing.

  #+begin_quote
  Slogan 0: Monoids model untyped programming, categories model typed programming.
  #+end_quote

  A common task for a programmer is to make inferences about large data sets.
  One begins with an empty database then aggregates data ---here's a monoid!
  Does it matter /when/ inferences are made? If we have no data, then we should be able
  to make no new, useful, inference. If we have we have two data-sets $x$ and $y$,
  we may aggregate them to produce a new data-set ~x â¨¾ y~ from which we may make an
  inference ~ğ‘°(x â¨¾ y)~. If we have multiple people working together, it may be ideal
  to make inferences ~ğ‘° x~ and ~ğ‘° y~ in â€˜parallelâ€™ then combine the inferences together
  to obtain a new inference ~ğ‘° x â¨¾â€² ğ‘° y~. Of-course, we would like these different approaches
  to yield the same results, which they may not. In the case they do, we say
  ~ğ‘° : DataSets â†’ Inferences~ is a /(homo)morphism/.

  As â€˜(homo)morphismâ€™, the name of structure-preserving operations between monoids, suggests,
  we have a category whose objects consist of monoids! Formally, a morphism of monoids
  ~ğ‘° : M âŸ¶ N~ is a function ~Carrier M â†’ Carrier N~ that sends ~Id M~ to ~Id N~ and
  distributes over composition in ~M~ to obtain composition in ~N~.

  The typed analogue of homomorphism is called a â€˜functorâ€™.
  A functor ~F = (Fâ‚€, Fâ‚) : ğ“’ âŸ¶ ğ’Ÿ~ between categories is a homomorphism ---in that it must preserve
  the identities and the compositional structure--- and it must also preserve the typing:
  If ~f : A âŸ¶ B~ in ğ’, then ~Fâ‚ f : Fâ‚€ A âŸ¶ Fâ‚€ B~ in ğ’Ÿ. For example, if ğ“’ were Haskell
  and ğ’ where C, and ~Fâ‚€~ was an assignment of Haskell types to C types, then ~Fâ‚~ must
  preserve typing in that the type of a transformed program ~Fâ‚ f~ is obtainable from
  the type of the original program ~f~. It is traditional to drop the subscripts and refer
  to ~Fâ‚€, Fâ‚~ simply by ~F~ ---contextual inference eliminates any ambiguity.

  Since categories have nebulous objects, functors also act as a means to endow
  objects with structure. For example, pairs of integers are just integers under
  the â€œpairs ofâ€ operation. More formally, let ~Binâ‚€ A = A Ã— A = { (x, y) | x, y âˆˆ A }~
  and ~Binâ‚ f (x, y) = (f x, f y)~.
  Since ~Binâ‚ id (x, y) = (id x, id y) = (x , y) = id (x, y)~
  and ~Binâ‚ (f â¨¾ g) (x, y) = ( g (f x), g (f y) ) = (Binâ‚ f â¨¾ Bin g) (x, y)~,
  one easily finds ~Bin = (Binâ‚€, Binâ‚)~ to be a functor that forms pairs.
  The operation of forming pairs in general, for any ~A~ and ~B~,~ is a bifunctor
  denoted â€œÃ—â€, with no subscripts as in ~A Ã— B~ and ~f Ã— g~.
  Likewise, lists of characters are just characters under the â€œlists ofâ€ operation.
  We encourage the reader to formalise the lists functor and check that it, and the pairs functor,
  indeed satisfy the functor laws.

  #+begin_quote
  Slogan 1: Homomorphisms are structure-preserving operations, functors are homomorphisms
  that also preserve typing. Moreover, functors are structure.
  #+end_quote

  Any general programming language worth its name would support a form of polymorphism
  ---the ability to declare a cookie-cutter recipe applicable to a family of different types.
  Monoids no longer provide for such an abstraction ---they have no notion of types, how could they!
  Consider the doubling functions:
  #+BEGIN_SRC haskell
  dupInt : Int â†’ Int Ã— Int
  dupInt x = (x , x)

  dupChar : Char â†’ Char Ã— Char
  dupChar c = (c, c)
  #+END_SRC
  We are forced to repeat the definition for each type we are interested in.
  Polymorphic functions remedy this shortcoming by allowing a
  /write once, use many/ approach:
  #+BEGIN_SRC haskell
  {- A natural transformation from the identity functor to the pairing functor -}
  dup : âˆ€{A} â†’ A â†’ A Ã— A
  dup a = (a , a)
  #+END_SRC
  When such a function makes /no dependence on A/, we say that it is
  /parametric polymorphic/ and that it comes with an optimisation law
  known as â€˜naturalityâ€™: For any operation ~f~ we have ~(f Ã— f) âˆ˜ dup  = dup âˆ˜ f~.
  Less cryptically, this says that expressions of the form
  ~let (x, y) = dup a; (xâ€², yâ€²) = (f x, f y) in â‹¯~
  may be replaced with the ~let aâ€² = f a; (xâ€² , yâ€²) = dup aâ€² in â‹¯~.
  However, polymorphism does not always come in this form; for example,
  languages such as ~C#~ which allow type inspection would allow us to form
  the following polymorphic method which is not natural:
  #+BEGIN_SRC haskell
  first : âˆ€{A} â†’ A Ã— A â†’ A
  first {A} (x, y) = if A is Int then 0 else x
  #+END_SRC

  A /natural transformation/ is a family ~Î· : âˆ€{A} : F A âŸ¶ G A~
  of morphisms such that for any $f : A âŸ¶ B$ we have $F f â¨¾ Î·_B = Î·_A â¨¾ G f$.
  If one thinks of ~F~ and ~G~ as structures or formats, then ~Î·~ is tantamount to uniform restructuring.
  Less cryptically, the constraint says that the ways to â€˜renameâ€™, â€˜relabelâ€™, â€˜transformâ€™ ~F A~ to ~G B~
  using any ~f~, are identical: We may rename by operating over the F-structure then reorganise using $Î·$,
  or reorganise using $Î·$ first then rename by operating over the resulting G-structure.

  The reader is encouraged at this point to find an ~f : A â†’ B~ showing
  that ~first~ above fails to be a natural transformation.

  #+begin_quote
  Slogan 2: Functors are structures and natural transformations are uniform restructuring schemes.
  #+end_quote

  A program ~A âŸ¶ B~ can be thought of as the careful manipulation of ~A~-data to yield ~B~-data.
  When ~A~ and ~B~ are different representations of the same data, the operation is invertible.
  Just as categories model typed programming languages, the notion of non-lossy protocols
  is modelled by the concept of /isomorphism/. An isomorphism, denoted ~A â‰… B~,
  is a pair of morphisms ~f : A âŸ¶ B~ and ~g : B âŸ¶ A~ that â€œundoâ€ each other: ~f â¨¾ g = Id = g â¨¾ f~.
  More concretely, this condition becomes:
  \[
  \forall a, b \bullet\qquad f\, a = b \quad\equiv\quad a = g\, b
  \]
  More often than nought, programs ~A âŸ¶ B~ are not invertible but do have a /best approximate inverse/.
  One writes $f âŠ£ g$ to indicate this relationship. When there are notions of â€˜approximationâ€™,
  denoted â€˜â‰¤â€™, the constraint becomes:
  \[
  \forall a, b \bullet\qquad f\, a â‰¤_B b \quad\equiv\quad a â‰¤_A g\, b
  \]
  Strict equalties have been replaced with approximations instead.

  For example, the injection $â„¤ â†ª â„$ and the ~dup~-lication function
  from earlier have no inverse. However, they do have best approximate inverses:
  \begin{align*}
     & âŒˆrâŒ‰ â‰¤_â„¤ n \quad\equiv\quad r â‰¤_â„ n
  \\ & x â†‘ y â‰¤_â„ z \quad\equiv\quad (x, y) â‰¤_{â„ Ã— â„} \mathsf{dup}\, z
  \\ & p âˆ§ q â‡’ r \quad\equiv\quad (p â‡’ r) âˆ§ (q â‡’ r) \text{ i.e., } (p, q) â‡’_{ğ”¹ Ã— ğ”¹} \mathsf{dup}\, r
  \end{align*}

  + The ceiling $âŒˆrâŒ‰$ of a number is the largest /whole/ number that is approximated by $r$:
    0. It is a whole number, $âŒˆ\_{}âŒ‰ : â„ â†’ â„¤$.
    1. It is approximated by $r$: Taking $n â‰” âŒˆrâŒ‰$ in the characterisation yields
      $r â‰¤ âŒˆrâŒ‰$.
    2. If $r$ approximates another number, say $n$, then $âŒˆrâŒ‰$ approximates it too!
       This is just the â€˜â‡â€™ reading of the characterisation.
  + The maximum xâ†‘y is the largest /single/ number that is approximated by both /x/ and /y/.
  + The conjunction $p âˆ§ q$ is the largest /single/ Boolean approximating both Booleans $p$ and $q$.

  Generalising on the duplication example, the reader is encouraged to verify $\sup âŠ£ K$,
  where the constant function $K : â„ â†’ (â„ â†’ â„)$ takes an element $z$ to the function $(K z) x = z$
  and $\sup : (â„ â†’ â„) â†’ â„$ takes a function $f$ to its supremum $\sup f$.

  #+begin_quote
  Slogan 3: For familiar or simple $g$, one can find useful or complex $f$ with ~f âŠ£ g~.
  #+end_quote

  For two types ~A~ and ~B~, there may be a number of ways that one â€œapproximatesâ€ the other.
  In a category, we may simply say any morphism ~f : A âŸ¶ B~ witnesses such an approximation.
  With this in-hand, the previous formulation lifts to the categorical setting as follows.
  For functors ~L : ğ’ âŸ¶ ğ’Ÿ : R~, one says that /L is adjoint to R/, denoted ~L âŠ£ R~, when
  there is an isomorphism, as follows, natural in ~A, B~.
  \[
  \forall A, B \bullet\qquad L\, A âŸ¶_ğ’Ÿ B \quad\equiv\quad A â‰¤_ğ’ R\, B
  \]

  This formulation is terse and easily motivated from the simpler setting,
  however for verification purposes it is a bit difficult to work with.
  There is a more â€˜localâ€™ formulation.

  An /adjunction/ $L âŠ£ R$ consists of two (not necessarily natural!) transformations
  $Î· : Id â†’ RL$ and $Îµ : LR â†’ Id$ such that
  \[
  \forall f, g \bullet\qquad f = Î· â¨¾ R\, g \quad\equiv\quad L\, f â¨¾ Îµ = g
  \]
   Recall that we may construe functors $F$ as structure, then
   maps $X â†’ F X$ provide ways to produce structured elements and so are
   referred to as /F-algebras/./ In particular, transformations $X â†’ F X$ may be
   thought of as injecting or â€œboxing upâ€ elements with a trivial F-structure
   whereas transformations $F X â†’ X$ can be thought of as â€œcompiling downâ€
   the structure to obtain a concrete value.

   With such a terminology, the above characterisation reads:
   /Each L-algebra g is uniquely determined ---as an L-map followed by an Îµ-reduction---
   by its restriction to the unit Î·./ Later in the setting of monoids and lists,
   this becomes: List homomorphisms are uniquely determined, as a map followed by a
   reduce, by their restriction to the singleton lists.

   It can be shown that the transformations are actually natural.
   As such, a more local formulation of $L âŠ£ R$ consists of a pair of natural
   transformations $Î· : Id â†’ RL$ and $Îµ : LR â†’ Id$ such that the â€œzig-zagâ€ laws
   holds: $Id = Î· â¨¾ RÎµ$ and $Id = LÎ· â¨¾ Îµ$. This is the formulation we shall follow
   in the remainder of the exposition.

   The remainder of the exposition moves slogan 3 from primitive types to the
   more complex types that programmers are generally interested in.
   We begin with simple theories, form the forgetful functor $R$, then seek
   to find the free functor $L$. In the process of establishing the adjunction $L âŠ£ R$
   we are forced to construct the following tool-kit:

   + Type Constructor ::
        We have a type constructor $L$ that furnishes raw types with structure.
   + Map ::
        Functions $A â†’ B$ can be lifted to work on L-structures
        yielding maps $L\, A â†’ L\, B$. In database/C# settings, this is known as
        ~select~. Moreover, this operation is a homomorphism. In Haskell notation,
     - ~map id = id~
     - ~map (f â¨¾ g) = map f â¨¾ map g~
   + Wrap :: We obtain a way Î· to construe raw data as having â€˜singletonâ€™ structure.
   + Interpreter :: We obtain a way Îµ to (recursively) â€œfoldâ€ over a structured
                    value to obtain a single value.

                    Moreover, the zig-zag laws ensure that forming a singleton
                    then reducing it is a no-op, as expected.

* Theories Yield Data-Structures

 :JC:
 I believe (but am quite willing to discuss this) that we should take theories as primary, and structure the paper around theories and their names.

What we should not do then, is structure things around our interpretation of the data-structure underlying the 'free theories' (and their names). Yes, we do need to provide that interpretation, it is a useful result. But the exposition of the ideas need to have an order. And I believe that

Theory --> Free Theory --> Interpretation of underlying data-structure
is the right order in which to expose the ideas.
 :End:

 Since our readership is intended to be a computing audience, there is little
 to be gained by starting out with data-structures then observing they have a particular
 algebraic structure. Instead, we propose to begin with tremendously simple interfaces
 which may arise in computing situations, then briefly form outline their theory.
 From there, we may then obtain a free theory and discuss its interpretation
 as a familiar data-structure.

 Let us begin with a situation that one encounters almost immediately when learning
 programming: What default value should you use when writing a loop to aggregate
 the values in some structure? For example, when multiplying or adding values in a list one
 may use 1 or 0, respectively, as an initial value for the aggregation ---which then acts
 as the default value for empty lists. Likewise, we may wish to implement a â€œsafe headâ€
 operation: Given a list of numbers, we return the first number if it is non-empty and
 otherwise we return 0 or 1? Hard-coding the default return value forces users to form
 superfluous conditional expressions, as such it may be better to request a default value
 to begin with. This gives rise to the following program.

#+BEGIN_SRC haskell
{- Types that have a â€˜defaultâ€™ value -}
record Pointed : Setâ‚ where
  field
    Carrier : Set
    Point   : Carrier

open Pointed

safe-head : {P : Pointed} â†’ List (Carrier P) â†’ Carrier P
safe-head {P} []       = Point P
safe-head {P} (x âˆ· xs) = x
#+END_SRC

 Now that we have demonstrated the utility of the interface, let us analyse its theory.

 A â€œpointed theoryâ€ consists of a type along with an elected value of that type;
 there are no further constraints. A homomorphism, or structure-preserving function,  ~P âŸ¶ Q~
 of pointed theories is a function between the carries that sends the point of ~P~ to the
 point of ~Q~. For example, lists over any type form a pointed theory with the point being
 the empty list, and so the above ~safe-head~, by definition, is actually a homomorphism.
 It is instructive to verify that the identity function is a homomorphism
 and that the composition of homomorphisms is again a homomorphism.
 Consequently, pointed theories form a category that we shall call ğ’«ğ’®â„¯ğ“‰.

 If we drop the point from a pointed theory, we are left with a type and so have
 a â€˜forgetful functorâ€™, say, $â„› : ğ’«ğ’®â„¯ğ“‰ âŸ¶ ğ’®â„¯ğ“‰$. As per our slogans, let us
 search for a functor â„’ that is left adjoint to â„›.
 Rather than guess an â„’, we shall attempt to calculate it by using the adjunction
 characterisation. Indeed, for any $X, Y, y$, we aim to find an â„’ such that
 $â„’ X âŸ¶ (Y, y) \quadâ‰…\quad X â†’ â„›(Y, y)$:

# Readme: http://ctan.math.ca/tex-archive/macros/latex/contrib/calculation/calculation.pdf
#+LATEX_HEADER: \usepackage{calculation}

#
#+begin_export latex
\begin{calculation}
    f \in X â†’ â„› (Y, y)  \comment{ ---In category $ğ’®â„¯ğ“‰$ }
\step[â‰¡]{ Definition of â„› }
    f \;\in\; X â†’ Y
\step[â‰…]{ Wish we could transform $f â†¦ fâ€², X â†¦ Xâ€²$ and have $xâ€² : Xâ€²$ }
    fâ€² \;\in\; Xâ€² â†’ Y \quad\land\quad fâ€²\, xâ€² = y
\step[â‰¡]{ Definition of homomorphism }
    fâ€² \;\in\; (Xâ€², xâ€²) âŸ¶ (Y, y)
\step[â‰¡]{ \textbf{Define} $â„’\, X = (Xâ€², xâ€²)$ }
    fâ€² \;\in\; â„’\, X âŸ¶ (Y, y)  \comment{ ---In category $ğ’«ğ’®â„¯ğ“‰$ }
\end{calculation}
#+end_export
To completely define â„’, it seems we must associate to any set $X$ a new non-empty
set $Xâ€²$ such that functions $X â†’ Y$ correspond exactly to homomorphisms $(Xâ€², xâ€²) âŸ¶ (Y, y)$. The simplest thing to do is to adjoin $X$ with a new formal element,
then $fâ€²$ is defined by sending the new element to $y$ and otherwise acts like $f$.
Conversely, given such an $fâ€²$ we regain $f$ by simply embedding the existing elements
of $X$ into $Xâ€²$. We may code up $Xâ€², fâ€²$ as ~Maybe X, âŸ¦ f âŸ§~ below, and the inverse
operation as ~lower f~.

#+BEGIN_SRC haskell
data Maybe (X : Set) : Set where
  Just    : X â†’ Maybe X   {- Keeping existing elements of X -}
  Nothing : Maybe X       {- Adjoining a new element -}

âŸ¦_âŸ§ : {X : Set} {Y : Pointed} (f : X â†’ Carrier Y) â†’ Maybe X â†’ Carrier Y
âŸ¦ f âŸ§ Nothing  = Point Y  {- The new forced equation for the new element -}
âŸ¦ f âŸ§ (Just x) = f x      {- Leaving f to behave on existing elements    -}

lower : {X : Set} {Y : Pointed} (fâ€² : Maybe X âŸ¶ Y)  â†’  X â†’ Y
lower fâ€² x = fâ€² (Just x)  {- Embed existing elements of X into Xâ€² -}
#+END_SRC

Clearly these operations are inverses and being parametric polymorphic,
they are natural transformations. However, for â„’ to be a functor we also
need to define its behaviour on functions, which we do below via ~map~,
and so we have the necessary pieces for the adjunction.
In verifying the full details of $â„’ âŠ£ â„›$, we find that
/every pointed homomorphism is determined uniquely as ~map~ followed by an interpretation âŸ¦âŸ§!/

#+BEGIN_SRC haskell
map : {A B : Set} â†’ (A â†’ B) â†’ (Maybe A â†’ Maybe B)
map f Nothing  = Nothing
map f (Just a) = Just (f a)
#+END_SRC

It is at this stage that we may enumerate the benefits of exploring the simple theory
of pointed structures. The goal of finding an adjoint for discarding the pointed structure
has resulting in the following useful toolkit.

0. The ~Maybe~ data-structure: When one is unsure what would be a useful
   default value to return, they may simply return ~Nothing~.

   This provides a standard mechanism to handle unexpected situations:
   Rather than crash the system or request a default value,
   yield a formal value and let the user handle it.
   In particular, object-oriented languages users, such as of C# and Java,
   do this all the time by returning ~null~ as a default value.

0. We have elected to use the name ~Maybe~; however this data-structure arises under
   a number of different names, according to intended usage. Names include:

   - â€˜Nullableâ€™: In object-oriented settings, it denotes a value that is completely
      â€˜undefinedâ€™ but in a first-class fashion.

   - â€˜Optionalâ€™: In an method argument location, it denotes arguments that need not be
     present.

0. The â€˜interpretationâ€™ operation âŸ¦âŸ§ is the method to coerce the formal element
   into a legitimate element of another data-type.

   # Being a natural transformation, it satisfies the following optimisation law
   # ~âŸ¦ â„› f âˆ˜ g âˆ˜ h âŸ§ = f âˆ˜ âŸ¦ g âŸ§ âˆ˜ â„’ h~.

0. We have a traversal function, ~map~, for working over such structures.

   It comes with two immediate optimisation rules: ~map id = id~
   and ~map (f âˆ˜ g) = map f âˆ˜ map g~.

The useful observation and primary contribution of our work is that a data-structure
/along/ with a minimal infrastructure â€œfalls outâ€ of the adjunction details for discarding
additional complexity.

* COMMENT DynamicalSystems ---Pointed Unary Theories

  A /pointed unary theory/ consists of a type, a default value of that type,
  and an operation on that type. Think of a box with a screen displaying
  the current state and a button that alters the state.
  Since such basic computing automata are an instance of such a theory
  and there is no standard name for the theory's operation, we shall
  refer to the operation as the â€œnext operationâ€ since it provides a next value
  in the type. Moreover, we may also refer to these theories as â€œdynamical systemsâ€
  since they mimic automata.

  Nearly any useful data-structure is an instance of this interface.
  As such, two simple examples more than suffice.
  + The naturals numbers â„• with starting state 0 and next operation
    being the successor function.
  + The automata with state space ~{even, odd}~, starting state ~even~, and next operation
    ~even â†¦ odd â†¦ even~. Consequently, the induced finite-state machine,
    ~foldl (const next) start xs~,
    informs us whether a string input ~xs~ has even or odd length.

#+latex: \def\next{\mathsf{next}}

 If we wish to take dynamical systems to be the objects of a category, call it ğ’Ÿğ’®,
 we must form a notion of homomorphism. The obvious thing to do is to say a
 homomorphism $h : X âŸ¶ Y$
 is a function between the state spaces that preserves the point
 ---i.e., it sends the default point of $X$ to the default point of /Y/---
 and it commutes with the â€˜nextâ€™ operation: $âˆ€ x â€¢\; h(\next_X\, x) = \next_Y (h\, x)$.
 It is then a simple exercise to show that the identity function is a homomorphism
 and the functional composition of homomorphisms results in a homomorphism.

 Let $â„› : ğ’Ÿğ’® â†’ ğ’®â„¯ğ“‰$ be the function that yields the underlying state space of
 a dynamical system. In particular, on objects it â€˜forgetsâ€™ the default point and the
 next operation, simply yielding a set. On homomorphisms, it forgets the
 structural-preservation proofs and simply yields a function on sets.
 This is our â€œforgetfulâ€ functor.

 How do we form a â€œfree functorâ€ $â„’ : ğ’®â„¯ğ“‰ â†’ ğ’Ÿğ’®$?
 We could serialise programs over dynamical systems by simply keeping track of the
 constructors for the default element and the next operation. Then we could interpret,
 execute, or run such a program later provided we have a way dyanmical system in hand.
 Whence, we consider forming /terms/ over dynamical systems:
#+BEGIN_SRC haskell
  {- Dynamic system terms over a variable set A. -}
  data Term (A : Set) where
    {- variables are terms -}
    inject  : A â†’ Term A
    {- Function â€œnamesâ€ applied to terms are again terms -}
    default : Term A
    next    : Term A â†’ Term A
#+END_SRC
 Let's provide a more informative renaming:
#+BEGIN_SRC haskell
data Possibly (A : Set) where
  never  : Possibly A
  now    : A â†’ Possibly A
  later  : Possibly A â†’ Possibly A
#+END_SRC
Well that is definitely interesting; it seems we have stumbled upon a modal-like
data-structure. A value of type ~Possibly A~ may ~never~ be obtained, or it can be
obtained ~now x~ or it is deferred to a later time ~later p~.
By traversing such abstract syntax trees and altering elements as one sees them,
we obtain a ~map~ operation that makes this type into a functor, call it â„’.
Moreover, for each type ~A~ this functor yields a dynamical system
~(Possibly A, never, later)~.

We are nearly done with our analysis of pointed unary theories.
To show that $â„’ âŠ£ â„›$, we need embedding and evaluation polymorphic functions.
#+BEGIN_SRC haskell
Î· : âˆ€ {D} â†’ D â†’ Possibly (States D)  {- â‰ˆ  Id D â†’ â„’ (â„› D)  -}
Î· = now

Îµ : âˆ€ {D} â†’ Possibly D â†’ States D    {- â‰ˆ â„› (â„’ D) â†’ Id D -}
Îµ {D} never      =  default D
Îµ {D} (now d)    =  d
Îµ {D} (later pd) =  next D (Îµ pd)
#+END_SRC
Since no type inspection is performed, there are easily shown to be natural
transformation. The zig-zag laws are equally trivial.

The type ~Possibly A~ consists of dynamical system terms /over/ the â€˜variable setâ€™ ~A~.
What if we considered closed terms; i.e., omitting variables altogether.
Then the injection ~now~ can never be invoked and so may be removed; along with
renaming the other constructors we obtain:
#+BEGIN_SRC haskell
data ğ’© where
  zero : ğ’©
  succ : ğ’© â†’ ğ’©
#+END_SRC
It seems that the natural numbers were not just an instance of dynamical systems
but rather were canonically so: This type contains the minimum to be considered
a dynamical system! There is the carrier state space ğ“, the default ~zero~, and
the next operation ~succ~. Moreover, its relationship to other dynamical systems
is that it is â€œinitialâ€: There is a homomorphism from it to any other dynamical system,
as follows.
#+BEGIN_SRC haskell
{- Essentially: succâ¿ zero â†¦ nextâ¿(default D) -}
for-loop : âˆ€ {D} â†’ ğ’© â†’ States D
for-loop {D} zero     =  default D
for-loop {D} (succ n) =  next D (for-loop)
#+END_SRC

Perhaps the last thing we would have expected would have been for the humble C-style
for-loop to appear.

In summary, by looking at the free structures of pointed unary theories we have obtained:
0. the ~Possibly~ data type, which is functorial;
1. ways to serialise programs over dynamical systems and evaluate them;
2. the natural numbers;
3. the for-loop.

* COMMENT Lists ---Monoidal Theories

  Unsurprising monoids along with monoid homomorphisms also form a category,
  call it â„³â„´ğ“ƒ.

  Let $â„› : â„³â„´ğ“ƒ âŸ¶ ğ’®â„¯ğ“‰$ be the functor that forgets the structure
  ---the composition operator and the no-op element--- to yield a set.
  On morphisms, it forgets the proofs to yield a function on sets.
  This is our forgetful functor.

  As we mentioned earlier, to find the free structure associated with this
  theory we turn to terms over the theory:
#+BEGIN_SRC haskell
{- Monoidal terms over â€˜variablesâ€™ A -}
data Term (A : Set) where
  {- Variables are terms -}
  inj  : A â†’ Term A
  {- Function â€œnamesâ€ applied to terms are terms. -}
  Id   : Term A
  _â¨¾_  : Term A â†’ Term A â†’ Term A
#+END_SRC
  The pointed unary setting was lawless, whereas monoids have laws
  stating how the pieces interact: Composition is associative with unit the no-op.
  In particular, ~inj x â¨¾ Id = inj x~  /should/ hold but it does not.
  This issue is the lack of canonicity: There are multiple forms for items that should be identical.

  If we view the monoid laws as rewrite rules, then it suffices to consider
  a type of only normal forms and have arbitrary terms rewrite down to them.
#+BEGIN_SRC haskell
 Id â¨¾ t      â†¦  t
 s  ; Id     â†¦  s
 (r â¨¾ s) â¨¾ t  â†¦  r â¨¾ (s â¨¾ t)
#+END_SRC
 With these rewrites, an arbitrary term reduces to the form
 ~inj x0 â¨¾ (inj x1 â¨¾ (inj x2 â¨¾ (â‹¯ â¨¾ inj xN)))~; this immediately suggests
 the so-called â€œcons listsâ€: We force this right-parenthesising by
 having the left be a raw element and the right be a complex element.
#+BEGIN_SRC haskell
data List (A : Set) where
  []  : List A
  _âˆ·_ : A â†’ List A â†’ List A
#+END_SRC

 Had we use the associativity rule as a rewrite rule
 the other way around, we would have obtained â€œsnoc listsâ€, which are lists
 with constant time access to the last element. Hence there are multiple
 /presentations/ of canonical terms, it is enough to pick one and continue with that.
 The composition operator is regained by rewriting down into the canonical form
 ---by discarding units and parenthesising right-wards:
#+BEGIN_SRC haskell
_++_ : âˆ€ {A} â†’ List A â†’ List B
[] ++ ys       = ys
(x âˆ· xs) ++ ys = x âˆ· (xs ++ ys)
#+END_SRC

Walking along the pointers of a linked-list and altering values as we see them
provides the ~map~ operation, which is also known as ~foreach~ loop in Java and imperative languages.
It is easily seen to be functorial and so we have a functor, call it $â„’ : ğ’®â„¯ğ“‰ âŸ¶ â„³â„´ğ“ƒ$.

In order to show that $â„’ âŠ£ â„›$, we need to formulate embedding and evaluation polymorphic functions.
#+BEGIN_SRC haskell
wrap : âˆ€ {M} â†’ Carrier M â†’ List (Carrier M)
wrap m = m âˆ· []

fold : âˆ€ {M} â†’ List (Carrier M) â†’ Carrier M
fold {M} []       = Id M
fold {M} (x âˆ· xs) = x â¨¾ fold xs where _â¨¾_ = _â¨¾_ M
#+END_SRC

Notice that there is only one closed canonical term:
If we take the variable set ~A~ to be empty, we can never invoke the ~_âˆ·_~ constructor
and so only have the one value ~[]~. If we instead consider the free structure over
a /singleton/ set, taking ~A~ to have one irrelevant value, and renaming, yields the naturals again:
#+BEGIN_SRC haskell
{- ğ’© â‰… List âŠ¤ -}
data ğ’© : Set where
  zero : ğ’©
  succ : ğ’© â†’ ğ’©
#+END_SRC

In summary, by considering the free structure associated with the most ubiquitous form
of composition we obtained:

1. The linked-list data-structures, forwards with cons and backwards with snoc;
2. The ~map~, or â€œforeachâ€, looping construct; along with its optimisation laws:
   ~map id = id~ and ~map (f â¨¾ g) = map f â¨¾ map g~;
3. The helpful ~wrap~ function that embeds a type into the assocaited type of lists;
4. The fold recursion scheme, which is essentially looping.
5. The adjunction property is tantamount to:
   List homomorphisms are uniquely determined, as a map followed by a
   reduce, by their restriction to the singleton lists.

All this from the tiny theory of monoids!

* COMMENT DataStructures âŠ£ Theories

Our [[https://github.com/JacquesCarette/TheoriesAndDataStructures][repository]] contains many worked out details of how simple theories give rise to
interesting or common data-structures. The previous two sections demonstrated the
general process, when possible, and there is little to be gained by such repetition.
Instead we shall settle for a listing of results followed by remarks about some
theories that gave us unexpected trouble.

+ Two Sorted :: A two sorted theory consists of just two types and nothing more.

                There are two forgetful functors, depending on which sort is kept.
                The free structure is then to keep the current sort and declare the
                new other sort to be empty.

                Interestingly, another way to â€˜forgetâ€™ the two sorts is to produce
                the Cartesian product, which is a single type. This gives rise to
                the pairing â€œÃ—â€ functor, whose left adjoint is then the duplication
                functor ---c.f., ~dup~ from earlier. Surprisingly, duplication,
                denote it by Î”, itself has left-adjoint: If we think of ~Î”A~ as
                â€˜forgettingâ€™ we had a single type and instead thinking we have two
                types, then given any two types, the free single type is obtained
                from their disjoint union.

                Whence, âŠ âŠ£ Î” âŠ£ Ã—.

                Notable programming combinators:
                - Records from products;
                - Projections and structural maps over products;
                - Enumerations from disjoint sums;
                - injections and structural maps over sums;
                - duplication combinators and optimisation laws
                  from naturality conditions.

+ Relations :: A /heterogenous relation/ is essentially a binary predicate.

               There are at least two forgetful functors to ğ’®â„¯ğ“‰, depending on whether
               we keep the source or the target of the relation. The free structures
               are, surprisingly, the empty relations.

               Upon further reflection, this is rather reasonable.
               A relation is essentially a graph and if we are given a set of vertices,
               then the smallest graph that contains such a set must be the empty graph
               on that vertex set.

+ Pointed :: A pointed theory consists of a type along with a single elected point.

             These model types with default values, as in the case in C#.

             The forgetful functor is obtained by dropping the point.
             The free structure is obtained by adjoining a type with a new
             formal element, sometimes called ~null~.

             Notable programming combinators:
             + Nullable types;
             + the Maybe monad

+ Unary ::  A unary theory is a type along with a single unary function on it.

            Dropping the function gives us a forgetful functor, whereas the free
            structure gives us a modal-like data-structure:
            #+BEGIN_SRC haskell
            data Eventually (A : Set) where
              now   : A â†’ Eventually A
              later : Eventually A â†’ Eventually A
            #+END_SRC
            This type appears silently in the form of ~A~ values tagged by natural
            numbers, since ~Eventually A â‰… A Ã— â„•~. It gives us a structure for indicating
            â€œhow many (delayed) stepsâ€ were needed before we obtained a value.

            Notable programming combinators:
            + A novel modal-like type;
            + The evaluator is iteration, ~laterâ¿ (now a) â†¦ fâ¿ a~, along with
              an array of useful utility properties required for the proof obligations.

+ Involutive :: An involutive theory consists of a type along with a unary function ~f~
                on that type such ~f âˆ˜ f = id~.

                Keeping only the type gives us a forgetful functor.
                The free structure is obtained by tagging elements with Booleans
                ---the involution then becomes negating the Boolean tag.

                Interestingly, there are /two adjunction proofs/ corresponding to
                whether we embed elements by tagging them with â€˜trueâ€™ or with â€˜falseâ€™.

                Notable programming combinators:
                + The Booleans;
                + Boolean negation to swap the tag;
                + ~map~ that works on the elements, regardless of the tag.

+ Indexed Unary :: An indexed unary theory consists of a sort along with an indexed family
                   of operations on it: There is a type ~Carrier~ and an indexing type ~I~
                   and a family of â€œactionsâ€ ~Op : {i : I} â†’ Carrier â†’ Carrier~.

                   These model weak forms of automata.

                   Keeping only the carrier set yields a forgetful functor.
                   The free structure on ~A~ is obtained by using ~NonEmptyLists A~ as
                   the carrier and ~A~ as the index set, with list concatenation as the
                   family of operators: For each ~a : A~, we have an action ~a âˆ·_~.

                   Notable programming combinators:
                   + Fold is a homomorphism from lists from the index set to an indexed
                     unary theory, over the same index set.
                   + Non-empty lists.

+ Magma  :: A magma is just a sort along with a binary operation.

            Dropping the operation yields a forgetful functor.
            The data type of binary trees provides a free structure.

            Notable programming combinators:
            + Binary tree data structure;
            + Recursion schemes over binary trees;
            + Many coherency laws on how the recursion schemes interact with one another.

+ N-ary :: For given natural number $N$, an /N/-ary theory consists of a type along with
           an endo-operation of $N$ arguments.

           Dropping the operation yields a forgetful functor, whereas
           a free structure is obtained by rose trees.

+ Semigroup :: A semigroup consists of a sort and an associative binary operation.

               Keeping only the carrier sort yields a forgetful operation,
               the type of non-empty lists provides a free structure.

               # There is no free functor from mamgmas to semigroups:
               # Non-associative operations do not â€˜extendâ€™ to associative ones!
               #

            Notable programming combinators:
            + Non empty lists data structure;
            + Catenation of non-empty lists, along with associativity proof;
            + Recursion schemes;


+ Monoid :: A monoid consists is a semigroup with a point that acts as the operation's unit.
            Keeping only the carrier yields a forgetful operation,
            whereas linked-lists provide a free structure.

+ Bag :: This is a monoid with the additional law that compositional order does not matter.

         Keeping only the carrier yields a forgetful operation,
         but there is no free structure in a constructive setting.

         Viewing the bag axioms as rewrite rules does not yield canonical forms;
         in particular the commutativity axioms provides a rule ~l â¨¾ r â†¦ r â¨¾ l~ that
         can be applied infinitely often.

In classical, non-constructive settings, bags and other theories admit free constructions:
One simply forms the terms over the theory than quotients by the equivalence relation
induced from the axioms. However, in computing, we want to be able to actually manipulate
particular data-values rather than consider nebulous equivalence classes.
It seems the leap is not that large, with /decidable equality/ in hand, we can form
a free structure for bags ---but we're no longer in ğ’®â„¯ğ“‰ and so no longer in the traditional domain of computing.

* TODO COMMENT OLD Abstract                                          :ignore:
:PROPERTIES:
:CUSTOM_ID: abstract
:END:

# Use:  x vs.{{{null}}} ys
# This informs LaTeX not to put the normal space necessary after a period.
#
#+MACRO: null  @@latex:\null{}@@

#+begin_abstract

*placeholder* We give a rational reconstruction of some common (and
not-so-common) data-structures that arise in functional
programming. Our categorical approach also leads us to defining
standard functions which ought to be in all reasonable libraries of
data-structures. Being systematic in the exploration of the design
space reveals quite a lot of structure and information about
data-structures and their origins.

#+begin_center org
#+begin_small
---Source: https://github.com/JacquesCarette/TheoriesAndDataStructures---
#+end_small
#+end_center
#+end_abstract

* STARTED COMMENT Introduction

It is relatively well-known in functional programming folklore that lists and monoids
are somewhow related. With a little prodding, most functional programmers will recall
(or reconstruct) that lists are, in fact, an instance of a monoid. But when asked if there
is a deeper relation, fewer are able to conjure up ``free
monoid''. Fewer still would be able formally prove this relation, in
other words, to actually fill in all the parts
that make up the adjunction between the forgetful functor from the category of monoids (and
monoid homomorphisms) and the category of types (and functions) and
the free monoid functor. To do so in full detail is, however, quite
informative --- and we will proceed to do so below.
{{{edcomm(MA, It is important to mention that this has been worked out in numerous
other writings. That this is not the prime novelty of the work. E.g.; when
a library claims to support X does it actually provide the necessaity â€˜kitâ€™ that
that X /intersincly/ comes with?
)}}}

So as to never be able to cheat, cut corners, etc, we will do all of
our work in Agda, with this document[fn:1]
being literate (and, in fact, written in ultra-literate style via org-mode).
But when we do, something interesting happens: we are forced to write
some rather useful functions over lists. Somehow ~map~,
~_++_~ and ~fold~ are all /required/.

But is this somehow a fluke? Of course not! So, what happens when we
try to explore this relationship?

A programmer's instinct might be to start poking around various
data-structures to try and see which also give rise to a similar
relation. This is a rather difficult task: not all of them arise this
way. Instead, we start from the opposite end: systematically write
down ``simple'' theories, and look at what pops out of the
requirements of having a ``left adjoint to the forgetful
functor''. This turns out to be very fruitful, and the approach we
will take here.

Naturally, we are far from the first to look at this. {{{edcomm(JC, Fill
in the related work here. From Universal Algebra through to many
papers of Hinze, Gibbons, etc)}}}. In other words, the \emph{theory}
behind what we'll be talking about here is well known.

So why bother? Because, in practice, there is just as much beauty in
the details as there was in the theory! By \emph{systematically} going
through simple theories, we will create a dictionary between theories
and a host of useful data-structures. Many of which do not in fact
exist in the standard libraries of common (and uncommon) functional
languages. And even when they do exist, all the ``kit'' that is derived
from the theory is not uniformly provided.

Along the way, we meet several roadblocks, some of which are rather
surprising, as results from the (theory) literature tell us that there
really ought to be no problems there. Only when we dig deeper do we
understand what is going on: classical mathematics is not
constructive! So even when type theorists were busy translating
results for use in functional programming, by not actually proving
their results in a purely constructive meta-theory, they did not
notice these roadblocks. {{{edcomm(MA, Nice!)}}}
Surmounting these problems will highlight how
different axioms, via their \emph{shape}, will naturally give rise to
data-structures easily implementable with inductive types, and which
require much more machinery.

In short, our contributions:
- a systematic exploration of the space of simple theories
- giving a complete dictionary
- highlighting the ``kit'' that arises from fully deriving all the
  adjunctions
- a survey of which languages' standard library offers what structures
  (and what kit)

* TODO COMMENT Monoids and lists

\edcomm{JC}{Give the full details}

#+BEGIN_SRC haskell
module POPL19 where

open import Helpers.DataProperties

open import Function using (_âˆ˜_)
open import Data.Nat
open import Data.Fin  as Fin hiding (_+_)
open import Data.Vec as Vec hiding (map)
open import Relation.Binary.PropositionalEquality
#+END_SRC

* TODO COMMENT Exploring simple theories
\edcomm{JC}{Not fully sure how to go about this, while staying
leisurely}

* TODO COMMENT Trouble in paradise
Commutative Monoid, idempotence, and so on.

* TODO COMMENT Survey of implementations

* TODO COMMENT We want to be systematic about

+ Exploring Magma-based theories :: see
    https://en.wikipedia.org/wiki/Magma_(algebra)
   where we want to at least explore all the properties that are
   affine.  These are interesting things said at
    https://en.wikipedia.org/wiki/Category_of_magmas which should be
   better understood.

+ Pointed theories ::

   There is not much to be said here. Although I guess 'contractible'
   can be defined already here.

+  Pointed Magma theories ::

   Interestingly, non-associative pointed Magma theories don't show up
   in the nice summary above.
   Of course, this is where Monoid belongs. But it is worth exploring
   all of the combinations too.

+ unary theories ::

   wikipedia sure doesn't spend much time on these (see
   https://en.wikipedia.org/wiki/Algebraic_structure)
   but there are some interesting ones, because if the unary operation
   is 'f' things like
   forall x. f (f x) = x
   is *linear*, because x is used exactly once on each side. The
   non-linearity of 'f' doesn't count (else associativity wouldn't work
   either, as _*_ is used funnily there too).  So "iter 17 f x = x" is a
   fine axiom here too. [iter is definable in the ground theory]

   This is actually where things started, as 'involution' belongs here.

    And is the first weird one.

+ Pointed unary theories :: E.g., the natural numbers

+ Pointer binary theories :: need to figure out which are expressible

+ more :: semiring, near-ring, etc. Need a sampling. But quasigroup
  (with 3 operations!) would be neat to look at.


Also, I think we want to explore
- Free Theories
- Initial Objects
- Cofree Theories (when they exist)

Then the potential 'future work' is huge. But that can be left for
later. We want to have all the above rock solid first.

* TODO COMMENT Relationship with 700 modules

To make it a POPL paper, as well as related to your module work, it is
also going to be worthwhile to notice and abstract the patterns. Such as
generating induction principles and recursors.

A slow-paced introduction to reflection in Agda: \\
https://github.com/alhassy/gentle-intro-to-reflection

* TODO COMMENT Timeline

Regarding POPL: \\
https://popl20.sigplan.org/track/POPL-2020-Research-Papers#POPL-2020-Call-for-Papers \\
There is no explicit Pearl category, nor any mention of that style.
Nevertheless, I think it's worth a shot, as I think by being systematic,
we'll "grab" in a lot of things that are not usually considered part of
one's basic toolkit.

However, to have a chance, the technical content of the paper should be
done by June 17th, and the rest of the time should be spent on the
presentation of the material.  The bar is very high at POPL.

* TODO COMMENT Task list items below
+ [X] JC start learning about org mode

+ [X] JC Figure out how to expand collapsed entries

+ [ ] JC See Â§4, first code block, of https://alhassy.github.io/init/ to setup ~:ignore:~ correctly on your machine.
      - This may require you to look at sections 2.1 and 2.2.

      This also shows you how to get â€˜mintedâ€™ colouring.

+ [ ] JC Write introduction/outline

+ [ ] MA To read:
  /From monoids to near-semirings: the essence of MonadPlus and Alternative/,
     https://usuarios.fceia.unr.edu.ar/~mauro/pubs/FromMonoidstoNearsemirings.pdf.

* DONE COMMENT Literate Agda in Org-mode

  JC, for now, use â€œhaskellâ€ labelled src blocks to get basic colouring, and I will demonstrate org-agda
  for you in person, if you like. Alternatively, I can generate coloured org-agda on my machine at the very end.

  + A basic setup for /actually/ doing Agda development within org-mode
    can be found at: https://alhassy.github.io/literate/

  + Example uses of org-agda include
    - https://alhassy.github.io/next-700-module-systems-proposal/PackageFormer.html ; also â‹¯.org
      * Shallow use of org-agda merely for colouring ;;  Prototype for Package Formers

    + Source mentions org-agda features that have not been pushed to the org-agda repo.

    - https://alhassy.github.io/PathCat/
      * Large development with categories ;; Graphs are to categories as lists are to monoids
    - https://github.com/alhassy/gentle-intro-to-reflection
      * Medium-sized development wherein Agda is actually coded within org-mode.

* COMMENT Making README.md                                                 :noexport:

#+NAME: make-readme
#+BEGIN_SRC emacs-lisp :results none
(with-temp-buffer
    (insert
    "#+EXPORT_FILE_NAME: README.md
     ,#+OPTIONS: toc:nil

     ,#+HTML: <h1> TheoriesAndDataStructures </h1>

Showing how some simple mathematical theories naturally give rise to some common data-structures.

Attempting to answer the following questions:

+ Why do lists pop-up more frequently to the average programmer than, say, their duals: bags?

+ More simply, why do unit and empty types occur so naturally? What about enumerations/sums and records/products?

+ Why is it that dependent sums and products do not pop-up expicitly to the average programmer? They arise naturally all the time as tuples and as classes.

+ How do we get the usual toolbox of functions and helpful combinators for a particular data type? Are they ``built into'' the type?

+ Is it that the average programmer works in the category of classical Sets,   with functions and propositional equality? Does this result in some ``free constructions'' not easily made computable since mathematicians usually work in the category of Setoids but tend to quotient to arrive in `Sets` ---where quotienting is not computably feasible, in `Sets` at-least; and why is that?

    ")
    (org-mode)
    (org-md-export-to-markdown)
)
#+END_SRC

* COMMENT Footnotes

[fn:1] Sources available at https://github.com/JacquesCarette/TheoriesAndDataStructures


* COMMENT footer                                                     :ignore:

f7 âˆ· make and open pdf
f8 âˆ· make readme

(load-file "~/org-agda-mode/org-agda-mode.el")
(load-file "~/org-agda-mode/literate.el")

# Local Variables:
# eval: (progn (org-babel-goto-named-src-block "make-acmart-class") (org-babel-execute-src-block) (outline-hide-sublevels 1))
# eval: (global-set-key (kbd "<f8>") (lambda () (interactive) (org-babel-goto-named-src-block "make-readme") (org-babel-execute-src-block) (outline-hide-sublevels 1)))
# eval: (global-set-key (kbd "<f7>") (lambda () (interactive) (org-babel-tangle) (async-shell-command (concat  "open " (org-latex-export-to-pdf) ))))
# End:

* COMMENT README â”€ JC & WK

  + ~C-c C-e~ then ~l o~ to produce the PDF from the org file.

    Or simply press ~f7~ if you've allowed the local variables in this file.

  + ~\edcomm{Person}{Comment}~ to make first-class literate comments:

    \edcomm{MA}{Please read this document; \newline thanks}
